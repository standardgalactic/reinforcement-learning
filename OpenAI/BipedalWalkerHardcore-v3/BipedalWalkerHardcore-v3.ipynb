{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BipedalWalkerHardcore-v3\n",
    "\n",
    "---\n",
    "In this notebook, you will implement a TD3 agent with OpenAI Gym's BipedalWalkerHardcore-v3 environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import time\n",
    "from PrioritizedReplayBuffer import PrioritizedReplayBuffer\n",
    "from Agent import TD3\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate the Environment\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = 'BipedalWalkerHardcore-v3'\n",
    "env_name = 'BipedalWalker-v3'\n",
    "random_seed = 0\n",
    "save_every = 500            # safe trained models after interval\n",
    "print_every = 10\n",
    "score_to_solve = 300.0\n",
    "directory = \"./preTrained/\" # save trained models\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "continue_training = False\n",
    "\n",
    "max_episodes = 20000        # max num of episodes\n",
    "max_timesteps = 2000        # max timesteps in one episode\n",
    "\n",
    "exploration_noise = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobias\\.conda\\envs\\pytorch\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "env.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train the Agent with TD3\n",
    "Run the code cell below to train the agent from scratch. You are welcome to amend the supplied values of the parameters in the function, to try to see if you can get better performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer prefilled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Deep Learning\\Reinforcement-Learning\\OpenAI\\BipedalWalkerHardcore-v3\\PrioritizedReplayBuffer.py:41: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.sample_index] = np.array(sample)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Average Score: -122.36, Max: -102.19, Min: -172.25, Time: 0.75\n",
      "Episode 20, Average Score: -105.52, Max: -100.21, Min: -115.54, Time: 0.86\n",
      "Episode 30, Average Score: -117.29, Max: -98.16, Min: -185.09, Time: 2.15\n",
      "Episode 40, Average Score: -109.98, Max: -99.98, Min: -122.49, Time: 0.87\n",
      "Episode 50, Average Score: -119.39, Max: -87.79, Min: -146.10, Time: 8.08\n",
      "Episode 60, Average Score: -121.35, Max: -93.65, Min: -164.09, Time: 1.80\n",
      "Episode 70, Average Score: -111.18, Max: -93.37, Min: -121.10, Time: 1.89\n",
      "Episode 80, Average Score: -112.57, Max: -107.77, Min: -118.87, Time: 1.02\n",
      "Episode 90, Average Score: -114.90, Max: -101.58, Min: -160.01, Time: 26.27\n",
      "Episode 100, Average Score: -122.92, Max: -108.48, Min: -135.46, Time: 27.17\n",
      "Episode 110, Average Score: -112.63, Max: -107.00, Min: -128.00, Time: 1.46\n",
      "Episode 120, Average Score: -114.53, Max: -102.59, Min: -157.48, Time: 1.17\n",
      "Episode 130, Average Score: -118.35, Max: -92.40, Min: -173.03, Time: 25.25\n",
      "Episode 140, Average Score: -95.62, Max: -48.71, Min: -145.32, Time: 24.92\n",
      "Episode 150, Average Score: -114.07, Max: -80.13, Min: -146.37, Time: 0.83\n",
      "Episode 160, Average Score: -129.11, Max: -26.57, Min: -214.24, Time: 1.13\n",
      "Episode 170, Average Score: -96.46, Max: -6.50, Min: -149.16, Time: 24.99\n",
      "Episode 180, Average Score: -108.34, Max: -52.17, Min: -131.20, Time: 1.27\n",
      "Episode 190, Average Score: -86.43, Max: -29.52, Min: -120.79, Time: 24.81\n",
      "Episode 200, Average Score: -106.19, Max: -25.49, Min: -150.75, Time: 25.02\n",
      "Episode 210, Average Score: -51.85, Max: 183.66, Min: -133.19, Time: 26.67\n",
      "Episode 220, Average Score: 67.19, Max: 237.16, Min: -116.73, Time: 1.66\n",
      "Episode 230, Average Score: 64.88, Max: 253.91, Min: -111.65, Time: 22.11\n",
      "Episode 240, Average Score: -106.20, Max: -48.19, Min: -120.12, Time: 1.80\n",
      "Episode 250, Average Score: -103.95, Max: -63.75, Min: -120.99, Time: 1.16\n",
      "Episode 260, Average Score: -61.38, Max: 272.79, Min: -108.81, Time: 0.83\n",
      "Episode 270, Average Score: -107.81, Max: -97.83, Min: -113.12, Time: 0.87\n",
      "Episode 280, Average Score: -57.85, Max: 185.84, Min: -148.34, Time: 7.69\n",
      "Episode 290, Average Score: -74.86, Max: 36.09, Min: -115.72, Time: 1.21\n",
      "Episode 300, Average Score: 116.04, Max: 259.78, Min: -89.07, Time: 1.80\n",
      "Episode 310, Average Score: 56.53, Max: 276.38, Min: -78.82, Time: 25.26\n",
      "Episode 320, Average Score: 54.00, Max: 276.16, Min: -123.62, Time: 11.75\n",
      "Episode 330, Average Score: 263.06, Max: 289.73, Min: 113.55, Time: 14.53\n",
      "Episode 340, Average Score: 19.04, Max: 285.21, Min: -108.28, Time: 1.95\n",
      "Episode 350, Average Score: 164.28, Max: 291.03, Min: -74.53, Time: 15.31\n",
      "Episode 360, Average Score: 170.49, Max: 290.08, Min: -16.97, Time: 8.54\n",
      "Episode 370, Average Score: 67.09, Max: 292.38, Min: -106.69, Time: 7.15\n",
      "Episode 380, Average Score: 105.50, Max: 288.82, Min: -109.43, Time: 14.34\n",
      "Episode 390, Average Score: 201.76, Max: 295.23, Min: -19.83, Time: 12.44\n",
      "Episode 400, Average Score: 170.82, Max: 293.38, Min: -30.71, Time: 6.88\n",
      "Episode 410, Average Score: 119.22, Max: 296.30, Min: -3.22, Time: 7.96\n",
      "Episode 420, Average Score: 246.72, Max: 294.46, Min: -114.17, Time: 12.72\n",
      "Episode 430, Average Score: 194.25, Max: 293.93, Min: -34.67, Time: 13.75\n",
      "Episode 440, Average Score: 65.75, Max: 297.57, Min: -111.98, Time: 8.95\n",
      "Episode 450, Average Score: 250.13, Max: 295.40, Min: -98.20, Time: 14.64\n",
      "Episode 460, Average Score: 181.13, Max: 295.67, Min: 12.22, Time: 8.82\n",
      "Episode 470, Average Score: 46.55, Max: 291.96, Min: -129.87, Time: 1.92\n",
      "Episode 480, Average Score: -102.97, Max: -23.59, Min: -135.44, Time: 1.57\n",
      "Episode 490, Average Score: -89.92, Max: -17.81, Min: -148.86, Time: 3.59\n",
      "Episode 500, Average Score: 86.36, Max: 284.04, Min: -88.58, Time: 15.68\n",
      "Episode 510, Average Score: 244.75, Max: 291.82, Min: 82.47, Time: 13.99\n",
      "Episode 520, Average Score: 74.59, Max: 284.43, Min: -96.17, Time: 14.20\n",
      "Episode 530, Average Score: 109.67, Max: 294.38, Min: -98.39, Time: 13.75\n",
      "Episode 540, Average Score: 159.43, Max: 295.29, Min: -30.54, Time: 11.14\n",
      "Episode 550, Average Score: 84.34, Max: 290.43, Min: -105.98, Time: 4.99\n",
      "Episode 560, Average Score: 163.67, Max: 291.09, Min: -43.93, Time: 10.65\n",
      "Episode 570, Average Score: 232.11, Max: 293.61, Min: -41.62, Time: 9.35\n",
      "Episode 580, Average Score: 127.87, Max: 290.88, Min: -108.30, Time: 14.15\n",
      "Episode 590, Average Score: 249.36, Max: 292.98, Min: 88.07, Time: 14.23\n",
      "Episode 600, Average Score: 156.13, Max: 294.97, Min: 6.11, Time: 5.18\n",
      "Episode 610, Average Score: 122.19, Max: 297.50, Min: -116.04, Time: 12.93\n",
      "Episode 620, Average Score: 106.99, Max: 297.23, Min: -81.84, Time: 4.77\n",
      "Episode 630, Average Score: 139.55, Max: 280.14, Min: -98.43, Time: 6.23\n",
      "Episode 640, Average Score: -52.27, Max: 57.15, Min: -87.78, Time: 3.76\n",
      "Episode 650, Average Score: -53.77, Max: 94.66, Min: -97.12, Time: 4.42\n",
      "Episode 660, Average Score: 135.97, Max: 268.30, Min: -206.04, Time: 6.05\n",
      "Episode 670, Average Score: 105.06, Max: 285.40, Min: -117.68, Time: 13.69\n",
      "Episode 680, Average Score: 178.42, Max: 289.74, Min: -41.42, Time: 17.30\n",
      "Episode 690, Average Score: 133.66, Max: 284.43, Min: -65.66, Time: 15.48\n",
      "Episode 700, Average Score: 130.74, Max: 287.25, Min: -109.85, Time: 0.97\n",
      "Episode 710, Average Score: 109.00, Max: 286.37, Min: -148.30, Time: 19.76\n",
      "Episode 720, Average Score: -20.13, Max: 260.19, Min: -101.88, Time: 1.94\n",
      "Episode 730, Average Score: 146.34, Max: 283.30, Min: -100.21, Time: 1.29\n",
      "Episode 740, Average Score: 93.76, Max: 284.04, Min: -109.21, Time: 14.48\n",
      "Episode 750, Average Score: 55.46, Max: 288.34, Min: -129.91, Time: 15.33\n",
      "Episode 760, Average Score: 202.12, Max: 279.95, Min: -63.44, Time: 14.12\n",
      "Episode 770, Average Score: 138.89, Max: 283.73, Min: -99.54, Time: 14.56\n"
     ]
    }
   ],
   "source": [
    "def train(buffer_prefill=10000):\n",
    "    policy = TD3(state_dim, action_dim, max_action)\n",
    "    \n",
    "    if continue_training:\n",
    "        policy.load(directory, filename)\n",
    "    \n",
    "    replay_buffer = PrioritizedReplayBuffer()\n",
    "    \n",
    "    # Prefill\n",
    "    state = env.reset()\n",
    "    while not len(replay_buffer) < buffer_prefill:\n",
    "        action = env.action_space.sample()\n",
    "        action = action.clip(env.action_space.low, env.action_space.high)\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "        replay_buffer.add((state, action, reward, next_state, float(done)))              \n",
    "        if done:\n",
    "            state = env.reset()            \n",
    "    print('Buffer prefilled')\n",
    "\n",
    "    scores = []\n",
    "    avg_reward = 0    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    ep_rewards_deque = deque(maxlen=100)\n",
    "\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        state = env.reset()\n",
    "        \n",
    "        ep_reward = 0\n",
    "        \n",
    "        timestep = time.time()\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            # select action and add exploration noise:\n",
    "            action = policy.select_action(state)\n",
    "            action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "            action = action.clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "            # take action in env:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "            \n",
    "            avg_reward += reward                       \n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # if i_episode is done then update policy:            \n",
    "            if (done or t==(max_timesteps-1)):\n",
    "                policy.update(replay_buffer, t)\n",
    "                break           \n",
    "                \n",
    "        episode_rewards.append(ep_reward)\n",
    "        ep_rewards_deque.append(ep_reward)        \n",
    "        avg_rewards = (avg_reward / print_every)\n",
    "        \n",
    "        if np.mean(ep_rewards_deque) >= score_to_solve:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, avg_rewards))\n",
    "            policy.save(directory, filename + '_solved')\n",
    "            break        \n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            min_rewards = np.min(episode_rewards)\n",
    "            max_rewards = np.max(episode_rewards)            \n",
    "            print('\\rEpisode {}, Average Score: {:.2f}, Max: {:.2f}, Min: {:.2f}, Time: {:.2f}'\\\n",
    "                  .format(i_episode, avg_rewards, max_rewards, min_rewards, time.time() - timestep), end=\"\\n\")\n",
    "            \n",
    "            avg_reward = 0\n",
    "            episode_rewards = []\n",
    "            \n",
    "        if i_episode % save_every == 0:\n",
    "            policy.save(directory, filename)\n",
    "        \n",
    "        scores.append(ep_reward)\n",
    "            \n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = train()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Watch a Smart Agent!\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(preTrained=True, max_timesteps=1500):        \n",
    "    n_episodes = 2    \n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])    \n",
    "    \n",
    "    if preTrained:\n",
    "        filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "        #filename += '_solved'    \n",
    "        directory = \"./preTrained/\"\n",
    "        policy = TD3(state_dim, action_dim, max_action)    \n",
    "        policy.load_actor(directory, filename)    \n",
    "        \n",
    "    for ep in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            if preTrained:\n",
    "                action = policy.select_action(state)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            env.render()            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "    env.close()\n",
    "        \n",
    "test()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
