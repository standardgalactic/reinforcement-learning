{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BipedalWalkerHardcore-v3\n",
    "\n",
    "---\n",
    "In this notebook, you will implement a TD3 agent with OpenAI Gym's BipedalWalkerHardcore-v3 environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import time\n",
    "from PrioritizedReplayBuffer import PrioritizedReplayBuffer\n",
    "from Agent import TD3\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate the Environment\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'BipedalWalkerHardcore-v3'\n",
    "#env_name = 'BipedalWalker-v3'\n",
    "random_seed = 34354\n",
    "save_every = 500            # safe trained models after interval\n",
    "print_every = 10\n",
    "score_to_solve = 300.0\n",
    "directory = \"./preTrained/\" # save trained models\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "continue_training = False\n",
    "\n",
    "buffer_init_size = 2**15\n",
    "max_episodes = 20000        # max num of episodes\n",
    "max_timesteps = 1500        # max timesteps in one episode\n",
    "\n",
    "exploration_noise = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobias\\.conda\\envs\\pytorch\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "env.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train the Agent with TD3\n",
    "Run the code cell below to train the agent from scratch. You are welcome to amend the supplied values of the parameters in the function, to try to see if you can get better performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer prefilled\n",
      "Episode 10, Average Score: -105.24, Max: -34.02, Min: -129.41, Time: 12.58\n",
      "Episode 20, Average Score: -120.07, Max: -94.92, Min: -137.56, Time: 12.85\n",
      "Episode 30, Average Score: -105.98, Max: -68.23, Min: -140.84, Time: 12.76\n",
      "Episode 40, Average Score: -105.18, Max: -81.84, Min: -139.26, Time: 13.07\n",
      "Episode 50, Average Score: -107.89, Max: -95.68, Min: -128.32, Time: 12.75\n",
      "Episode 60, Average Score: -110.55, Max: -104.87, Min: -117.45, Time: 12.72\n",
      "Episode 70, Average Score: -114.21, Max: -102.08, Min: -121.26, Time: 12.95\n",
      "Episode 80, Average Score: -115.39, Max: -107.47, Min: -118.69, Time: 12.80\n",
      "Episode 90, Average Score: -107.07, Max: -102.42, Min: -115.25, Time: 12.63\n",
      "Episode 100, Average Score: -103.20, Max: -101.06, Min: -107.60, Time: 12.82\n",
      "Episode 110, Average Score: -98.20, Max: -79.35, Min: -104.87, Time: 12.97\n",
      "Episode 120, Average Score: -106.53, Max: -98.43, Min: -126.90, Time: 12.85\n",
      "Episode 130, Average Score: -100.47, Max: -98.07, Min: -106.12, Time: 13.52\n",
      "Episode 140, Average Score: -110.19, Max: -98.81, Min: -151.48, Time: 14.63\n",
      "Episode 150, Average Score: -98.19, Max: -49.23, Min: -120.62, Time: 16.40\n",
      "Episode 160, Average Score: -116.36, Max: -98.19, Min: -162.81, Time: 13.57\n",
      "Episode 170, Average Score: -104.38, Max: -90.67, Min: -136.83, Time: 13.32\n",
      "Episode 180, Average Score: -110.09, Max: -89.02, Min: -158.84, Time: 14.25\n",
      "Episode 190, Average Score: -108.97, Max: -88.51, Min: -163.04, Time: 14.98\n",
      "Episode 200, Average Score: -92.19, Max: -50.68, Min: -117.28, Time: 15.10\n",
      "Episode 210, Average Score: -116.53, Max: -85.10, Min: -139.37, Time: 15.29\n",
      "Episode 220, Average Score: -98.96, Max: -57.24, Min: -133.59, Time: 13.25\n",
      "Episode 230, Average Score: -92.56, Max: -61.77, Min: -131.28, Time: 15.56\n",
      "Episode 240, Average Score: -93.95, Max: -55.51, Min: -156.62, Time: 15.07\n",
      "Episode 250, Average Score: -87.06, Max: -42.63, Min: -135.60, Time: 13.30\n",
      "Episode 260, Average Score: -108.43, Max: -68.92, Min: -141.97, Time: 13.15\n",
      "Episode 270, Average Score: -106.46, Max: -74.26, Min: -131.86, Time: 13.32\n",
      "Episode 280, Average Score: -106.81, Max: -65.92, Min: -130.82, Time: 13.05\n",
      "Episode 290, Average Score: -100.06, Max: -66.83, Min: -146.37, Time: 13.72\n",
      "Episode 300, Average Score: -116.41, Max: -102.89, Min: -141.58, Time: 13.27\n",
      "Episode 310, Average Score: -104.90, Max: -74.68, Min: -124.57, Time: 14.73\n",
      "Episode 320, Average Score: -105.03, Max: -81.32, Min: -124.73, Time: 12.92\n",
      "Episode 330, Average Score: -82.87, Max: -67.41, Min: -136.67, Time: 15.60\n",
      "Episode 340, Average Score: -93.10, Max: -67.03, Min: -124.02, Time: 12.73\n",
      "Episode 350, Average Score: -98.97, Max: -80.67, Min: -118.47, Time: 15.35\n",
      "Episode 360, Average Score: -101.75, Max: -87.94, Min: -115.34, Time: 14.78\n",
      "Episode 370, Average Score: -78.11, Max: -65.34, Min: -113.15, Time: 14.85\n",
      "Episode 380, Average Score: -71.43, Max: -61.58, Min: -83.43, Time: 14.44\n",
      "Episode 390, Average Score: -67.53, Max: -53.68, Min: -114.19, Time: 14.87\n",
      "Episode 400, Average Score: -70.89, Max: -55.91, Min: -122.52, Time: 14.80\n",
      "Episode 410, Average Score: -52.53, Max: -42.48, Min: -67.34, Time: 14.82\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    policy = TD3(state_dim, action_dim, max_action)\n",
    "    \n",
    "    if continue_training:\n",
    "        policy.load(directory, filename)\n",
    "    \n",
    "    replay_buffer = PrioritizedReplayBuffer()\n",
    "    \n",
    "    # Prefill\n",
    "    state = env.reset()\n",
    "    while not replay_buffer.is_full():\n",
    "        action = env.action_space.sample()\n",
    "        action = action.clip(env.action_space.low, env.action_space.high)\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "        replay_buffer.add((state, action, reward, next_state, float(done)))              \n",
    "        if done:\n",
    "            state = env.reset()            \n",
    "    print('Buffer prefilled')\n",
    "\n",
    "    scores = []\n",
    "    avg_reward = 0    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    ep_rewards_deque = deque(maxlen=100)\n",
    "\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        state = env.reset()\n",
    "        \n",
    "        ep_reward = 0\n",
    "        \n",
    "        timestep = time.time()\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            # select action and add exploration noise:\n",
    "            action = policy.select_action(state)\n",
    "            action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "            action = action.clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "            # take action in env:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "            \n",
    "            avg_reward += reward                       \n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # if i_episode is done then update policy:\n",
    "            if (done or t==(max_timesteps-1)):\n",
    "                policy.update(replay_buffer)\n",
    "                break\n",
    "                \n",
    "        episode_rewards.append(ep_reward)\n",
    "        ep_rewards_deque.append(ep_reward)        \n",
    "        avg_rewards = (avg_reward / print_every)\n",
    "        \n",
    "        if np.mean(ep_rewards_deque) >= score_to_solve:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, avg_rewards))\n",
    "            policy.save(directory, filename + '_solved')\n",
    "            break        \n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            min_rewards = np.min(episode_rewards)\n",
    "            max_rewards = np.max(episode_rewards)            \n",
    "            print('\\rEpisode {}, Average Score: {:.2f}, Max: {:.2f}, Min: {:.2f}, Time: {:.2f}'\\\n",
    "                  .format(i_episode, avg_rewards, max_rewards, min_rewards, time.time() - timestep), end=\"\\n\")\n",
    "            \n",
    "            avg_reward = 0\n",
    "            episode_rewards = []\n",
    "            \n",
    "        if i_episode % save_every == 0:\n",
    "            policy.save(directory, filename)\n",
    "        \n",
    "        scores.append(ep_reward)\n",
    "            \n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = train()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Watch a Smart Agent!\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(preTrained=True, max_timesteps=1500):        \n",
    "    n_episodes = 2    \n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])    \n",
    "    \n",
    "    if preTrained:\n",
    "        filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "        #filename += '_solved'    \n",
    "        directory = \"./preTrained/\"\n",
    "        policy = TD3(state_dim, action_dim, max_action)    \n",
    "        policy.load_actor(directory, filename)    \n",
    "        \n",
    "    for ep in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            if preTrained:\n",
    "                action = policy.select_action(state)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            env.render()            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "    env.close()\n",
    "        \n",
    "test()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
