{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BipedalWalkerHardcore-v3\n",
    "\n",
    "---\n",
    "In this notebook, you will implement a TD3 agent with OpenAI Gym's BipedalWalkerHardcore-v3 environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import time\n",
    "from PrioritizedReplayBuffer import PrioritizedReplayBuffer\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "from Agent import TD3\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate the Environment\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'BipedalWalkerHardcore-v3'\n",
    "#env_name = 'BipedalWalker-v3'\n",
    "random_seed = 0\n",
    "save_every = 500            # safe trained models after interval\n",
    "print_every = 10\n",
    "score_to_solve = 300.0\n",
    "directory = \"./preTrained/\" # save trained models\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "continue_training = True\n",
    "\n",
    "max_episodes = 20000        # max num of episodes\n",
    "max_timesteps = 2000        # max timesteps in one episode\n",
    "\n",
    "exploration_noise = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobias\\.conda\\envs\\pytorch\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "env.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train the Agent with TD3\n",
    "Run the code cell below to train the agent from scratch. You are welcome to amend the supplied values of the parameters in the function, to try to see if you can get better performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer prefilled\n",
      "Episode 10, Average Score: -95.98, Max: 14.20, Min: -189.05, Time: 23.48\n",
      "Episode 20, Average Score: -122.28, Max: -80.10, Min: -205.49, Time: 3.49\n"
     ]
    }
   ],
   "source": [
    "def train(buffer_prefill=10000):\n",
    "    policy = TD3(state_dim, action_dim, max_action)\n",
    "    \n",
    "    if continue_training:        \n",
    "        policy.load(directory, \"TD3_BipedalWalker-v3_0_solved\")\n",
    "    \n",
    "    #replay_buffer = PrioritizedReplayBuffer()\n",
    "    replay_buffer = ReplayBuffer()\n",
    "    \n",
    "    # Prefill\n",
    "    state = env.reset()\n",
    "    while not len(replay_buffer) < buffer_prefill:\n",
    "        action = env.action_space.sample()\n",
    "        action = action.clip(env.action_space.low, env.action_space.high)\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "        replay_buffer.add((state, action, reward, next_state, float(done)))              \n",
    "        if done:\n",
    "            state = env.reset()            \n",
    "    print('Buffer prefilled')\n",
    "\n",
    "    scores = []\n",
    "    avg_reward = 0    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    ep_rewards_deque = deque(maxlen=100)\n",
    "\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        state = env.reset()\n",
    "        \n",
    "        ep_reward = 0\n",
    "        \n",
    "        timestep = time.time()\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            # select action and add exploration noise:\n",
    "            action = policy.select_action(state)\n",
    "            action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "            action = action.clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "            # take action in env:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "            \n",
    "            avg_reward += reward                       \n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # if i_episode is done then update policy:            \n",
    "            if (done or t==(max_timesteps-1)):\n",
    "                policy.update(replay_buffer, t)\n",
    "                break           \n",
    "                \n",
    "        episode_rewards.append(ep_reward)\n",
    "        ep_rewards_deque.append(ep_reward)        \n",
    "        avg_rewards = (avg_reward / print_every)\n",
    "        \n",
    "        if np.mean(ep_rewards_deque) >= score_to_solve:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, avg_rewards))\n",
    "            policy.save(directory, filename + '_solved')\n",
    "            break        \n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            min_rewards = np.min(episode_rewards)\n",
    "            max_rewards = np.max(episode_rewards)            \n",
    "            print('\\rEpisode {}, Average Score: {:.2f}, Max: {:.2f}, Min: {:.2f}, Time: {:.2f}'\\\n",
    "                  .format(i_episode, avg_rewards, max_rewards, min_rewards, time.time() - timestep), end=\"\\n\")\n",
    "            \n",
    "            avg_reward = 0\n",
    "            episode_rewards = []\n",
    "            \n",
    "        if i_episode % save_every == 0:\n",
    "            policy.save(directory, filename)\n",
    "        \n",
    "        scores.append(ep_reward)\n",
    "            \n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = train()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Watch a Smart Agent!\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(preTrained=True, max_timesteps=1500):        \n",
    "    n_episodes = 2    \n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])    \n",
    "    \n",
    "    if preTrained:\n",
    "        filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "        filename += '_solved'    \n",
    "        directory = \"./preTrained/\"\n",
    "        policy = TD3(state_dim, action_dim, max_action)    \n",
    "        policy.load_actor(directory, filename)    \n",
    "        \n",
    "    for ep in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            if preTrained:\n",
    "                action = policy.select_action(state)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            env.render()            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "    env.close()\n",
    "        \n",
    "test()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
