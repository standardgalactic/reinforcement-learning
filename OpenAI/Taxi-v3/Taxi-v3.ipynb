{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "commercial-proportion",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "favorite-cathedral",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pylab as pl\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-prime",
   "metadata": {},
   "source": [
    "### Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "three-flood",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-thailand",
   "metadata": {},
   "source": [
    "### Inspect Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "integral-sharing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size  6\n",
      "State size  500\n"
     ]
    }
   ],
   "source": [
    "# There are 6 discrete deterministic actions:\n",
    "# - 0: move south\n",
    "# - 1: move north\n",
    "# - 2: move east\n",
    "# - 3: move west\n",
    "# - 4: pickup passenger\n",
    "# - 5: drop off passenger\n",
    "\n",
    "action_size = env.action_space.n\n",
    "print(\"Action size \", action_size)\n",
    "\n",
    "# There are 500 discrete states since there are 25 taxi positions\n",
    "# 5 possible locations of the passenger (including the case when the passenger is in the taxi)\n",
    "# and 4 destination locations.\n",
    "# Start-Position is random\n",
    "state_size = env.observation_space.n\n",
    "print(\"State size \", state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rocky-seventh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-invalid",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alternate-greek",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_actions, n_states, gamma=0.9):\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        \n",
    "    def decay_schedule(self, init_value, min_value, decay_ratio, max_steps, log_start=-2, log_base=10):\n",
    "        decay_steps = int(max_steps * decay_ratio)\n",
    "        rem_steps = max_steps - decay_steps\n",
    "        values = np.logspace(log_start, 0, decay_steps, base=log_base, endpoint=True)[::-1]\n",
    "        values = (values - values.min()) / (values.max() - values.min())\n",
    "        values = (init_value - min_value) * values + min_value\n",
    "        values = np.pad(values, (0, rem_steps), 'edge')\n",
    "        return values        \n",
    "            \n",
    "    def act(self, state, eps=0):\n",
    "        if random.uniform(0, 1) < eps:\n",
    "            return random.choice(np.arange(self.n_actions))        \n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done, alpha, algo='qlearn'):             \n",
    "        if algo == 'qlearn':            \n",
    "            #  Q-Learning\n",
    "            td_target = reward + self.gamma * np.max(self.Q[next_state, :]) * (not done)\n",
    "        \n",
    "        else:        \n",
    "            # SARSA\n",
    "            td_target = reward + self.gamma * self.Q[next_state, self.act(next_state)] * (not done)\n",
    "               \n",
    "        td_error = td_target - self.Q[state, action]  \n",
    "        \n",
    "        self.Q[state, action] = self.Q[state, action] + alpha * td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-label",
   "metadata": {},
   "source": [
    "### Q - Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "brilliant-scenario",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def learning(n_actions, n_states, episodes=50000, max_steps=500, print_every=5000):\n",
    "    agent = Agent(n_actions, n_states)\n",
    "    \n",
    "    alphas = agent.decay_schedule(0.9, 0.01, 0.2, episodes)\n",
    "    epsilons = agent.decay_schedule(1.0, 0.01, 0.5, episodes)\n",
    "    \n",
    "    for n_episode in range(episodes):\n",
    "        state = env.reset()        \n",
    "                \n",
    "        for n_step in range(max_steps):\n",
    "            action = agent.act(state, epsilons[n_episode])\n",
    "            next_state, reward, done, info = env.step(action)            \n",
    "            \n",
    "            agent.learn(state, action, reward, next_state, done, alphas[n_episode])\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:      \n",
    "                break\n",
    "        \n",
    "        if n_episode % print_every == 1:\n",
    "            print('Episode: {0} done after {1} Steps.'.format(n_episode+1, n_step))\n",
    "    \n",
    "    print('Done.')\n",
    "    env.close()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-charger",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reasonable-dimension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2 done after 199 Steps.\n",
      "Episode: 5002 done after 30 Steps.\n",
      "Episode: 10002 done after 12 Steps.\n",
      "Episode: 15002 done after 14 Steps.\n",
      "Episode: 20002 done after 5 Steps.\n",
      "Episode: 25002 done after 12 Steps.\n",
      "Episode: 30002 done after 10 Steps.\n",
      "Episode: 35002 done after 10 Steps.\n",
      "Episode: 40002 done after 11 Steps.\n",
      "Episode: 45002 done after 10 Steps.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "agent = learning(action_size, state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-truck",
   "metadata": {},
   "source": [
    "### Replay trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "closed-sport",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replay(agent, max_steps=20):    \n",
    "    n_steps = 0\n",
    "\n",
    "    state, done = env.reset(), False\n",
    "    rewards = 0\n",
    "\n",
    "    while not done and n_steps < max_steps:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        state = next_state\n",
    "        rewards += reward\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(.5)\n",
    "\n",
    "        n_steps+=1\n",
    "\n",
    "    print('Solved after {0} Steps.'.format(n_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "african-output",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Solved after 14 Steps.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    replay(agent)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45e531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
